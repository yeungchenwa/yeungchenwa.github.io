---
permalink: /
title: About Me
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<style>
body, h1, h2, h3, h4, h5, h6 {
  font-family: 'Times New Roman', Times, serif;
}
</style>

My name is Zhenhua Yang (æ¨æŒ¯å, Yeung Chenwa).
I'm a third-year Master's student from <a href="https://github.com/HCIILAB" style="text-decoration:none;">SCUT-DLVCLab</a> in <a href="https://www2.scut.edu.cn/ee/" style="text-decoration:none;">School of Electronic and Information Engineering</a>, <a href="https://www.scut.edu.cn/new/" style="text-decoration:none;">South China University of Technology</a>, supervised by <a href="http://www.dlvc-lab.net/lianwen/Index.html" style="text-decoration:none;">Prof. Lianwen Jin</a>. 
I received my Bachelor degree from <a href="https://www2.scut.edu.cn/automation/" style="text-decoration:none;">School of Automation Science and Engineering</a>, <a href="https://www.scut.edu.cn/new/" style="text-decoration:none;">South China University of Technology</a> in 2022. 

Previously, I have interned at <a href="https://klingai.com/global/" style="text-decoration:none;">Kling Team</a> of Kuaishou (advised by <a href="https://www.xtao.website/" style="text-decoration:none;">Xin Tao</a>)
and <a href="https://www.idea.edu.cn/" style="text-decoration:none;">International Digital Economy Academy (IDEA)</a> (advised by Prof. <a href="https://www.leizhang.org/" style="text-decoration:none;">Lei Zhang</a> and Dr. <a href="https://haozhang534.github.io/" style="text-decoration:none;">Hao Zhang</a>).

My research interests are focused on AIGC, Generative Model, and Large Multi-Modal Models. I am also devoted into the open source community. 

<!-- I am enthusiastic about discussing with different people. If you are interested, please feel free to $\color{#FF00FF}{contact\ me}$! -->
<!-- <p style="color: red;">[News] I will graduate in June 2025 and I am open to industrial research/engineering positions. If you are interested, please feel free to contact me.</p> -->

<!-- <p style="color: red;">I'm looking for a 2025Fall PhD position!</p> -->

<p align="center">
<a href='https://github.com/yeungchenwa'>GitHub</a> / 
<a href='https://scholar.google.com/citations?hl=zh-CN&user=2ITs6lUAAAAJ'>Google Scholar</a> / 
<a href='eezhyang@gmail.com'>Email</a> / 
<a href='https://www.zhihu.com/people/young-40-31'>Zhihu</a> / 
<a href='https://www.linkedin.com/in/zhenhua-yang-3911982b2'>Linkin</a>
</p>

News
-----
âˆ™ **[02/2025]** ğŸ‰ğŸ‰ğŸ‰ Our paper <a href="https://arxiv.org/abs/2412.11634" style="text-decoration:none;">HDR</a> is selected as the **oral** presentation <br />
âˆ™ **[12/2024]** The inference code of our paper <a href="https://arxiv.org/abs/2412.11634" style="text-decoration:none;">HDR</a> is released in <a href="https://github.com/yeungchenwa/HDR" style="text-decoration:none;">link</a>.<br />
âˆ™ **[12/2024]** Our paper <a href="https://arxiv.org/abs/2412.11634" style="text-decoration:none;">HDR</a> is accepted by **AAAI 2025** ğŸ‰ğŸ‰ğŸ‰, and the dataset, code and weight will be public soon. ğŸŒ¹ğŸŒ¹ğŸŒ¹<br />
âˆ™ **[07/2024]** I will attend to **ICML 2024** conference in person in Vienna, Austria. Open to have a disscussion or play with you. ğŸŒ¹ğŸŒ¹ğŸŒ¹<br />
âˆ™ **[06/2024]** Now I am interned at <a href="https://www.idea.edu.cn/" style="text-decoration:none;">International Digital Economy Academy (IDEA)</a>, supervised by Prof. <a href="https://www.leizhang.org/" style="text-decoration:none;">Lei Zhang</a> and closely work with Dr. <a href="https://haozhang534.github.io/" style="text-decoration:none;">Hao Zhang</a>, where I am working on the topic of vision-language large model for video understanding.<br />
âˆ™ **[05/2024]** Our paper <a href="https://arxiv.org/abs/2312.02694" style="text-decoration:none;">UPOCR</a> is accepted by ICML 2024 ğŸ‰ğŸ‰ğŸ‰.<br />
âˆ™ **[12/2023]** ğŸ”¥ğŸ”¥ğŸ”¥ The ğŸ“º<a href="https://huggingface.co/spaces/yeungchenwa/FontDiffuser-Gradio" style="text-decoration:none;">Hugging Face Demo</a> and the ğŸ§‘â€ğŸ’»<a href="https://github.com/yeungchenwa/FontDiffuser" style="text-decoration:none;">Github Repository</a> of <strong><a href='https://arxiv.org/abs/2312.12142'>FontDiffuser</a></strong> is released! Welcome to check it out.  
âˆ™ **[12/2023]** ğŸ‰ The paper <a href="https://arxiv.org/abs/2312.12142" style="text-decoration:none;">FontDiffuser</a> is accepted by <strong>AAAI2024</strong>, which excels in complex character generation and large style variation. The code and demo will be released soon.<br />
âˆ™ **[12/2023]** Our paper <a href="https://arxiv.org/abs/2312.02694" style="text-decoration:none;">UPOCR</a> is released to arXiv.<br />

Education
-----
<img style="float: left; margin:5px 10px" src="../my_images/SCUT_logo.png" width="100" height="100">
### South China University of Technology
<p style="line-height:1.1">
<font size="2">
Sep. 2022 - Present<br />
M.S student at <a href="https://github.com/HCIILAB" style="text-decoration:none;">SCUT-DLVCLab</a> in <a href="https://www2.scut.edu.cn/ee/" style="text-decoration:none;">School of Electronic and Information Engineering</a><br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/SCUT_logo.png" width="100" height="100">
### South China University of Technology
<p style="line-height:1.1">
<font size="2">
Sep. 2018 - Jun. 2022 <br />
B.E student in <a href="https://www2.scut.edu.cn/automation/" style="text-decoration:none;">School of Automation Science and Engineering</a><br />
</font>
</p>

Experience
-----
<img style="float: left; margin:5px 10px" src="../my_images/experiences/kling.png" width="100" height="100">
### <a href="https://klingai.com/global/" style="text-decoration:none;">KuaiShou - Kling Team</a>
<p style="line-height:1.1">
<font size="2">
Jan. 2025 - Apr. 2025 <br />
Research Intern <br />
Unifying Model for Generation and Understanding, supervised by <a href="https://klingai.com/global/" style="text-decoration:none;">Xin Tao</a>.<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/experiences/idea.png" width="100" height="100">
### <a href="https://www.idea.edu.cn/" style="text-decoration:none;">International Digital Economy Academy (IDEA)</a> - <a href="https://github.com/IDEA-Research" style="text-decoration:none;">CVR</a>
<p style="line-height:1.1">
<font size="2">
Jun. 2024 - Sep. 2024 <br />
Research Intern <br />
Streaming Video Captioning and Understanding / Region Caption, supervised by Prof. <a href="https://www.leizhang.org/" style="text-decoration:none;">Lei Zhang</a>.<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/experiences/intsig.png" width="100" height="100">
### <a href="https://www.intsig.com/" style="text-decoration:none;">INTSIG</a> - <a href="https://www.intsig.com/personal-camscanner" style="text-decoration:none;">CamScanner</a>
<p style="line-height:1.1">
<font size="2">
Mar. 2024 - May 2024 <br />
Engineering Intern <br />
Editing documents in real-world scenarios.<br />
</font>
</p>

Publications
-----
<img style="float: left; margin:5px 10px" src="../my_images/publications/HDR.png" width="160" height="160">
### Predicting the Original Appearance of Damaged Historical Documents
<p style="line-height:1.1">
<font size="2">
<strong>Zhenhua Yang*</strong>, <a href="https://scholar.google.com/citations?user=6zNgcjAAAAAJ&hl=zh-CN&oi=ao" style="text-decoration:none;">Dezhi Peng</a>*, Yongxin Shi, Yuyi Zhang, Chongyu Liu, <a href="http://www.dlvc-lab.net/lianwen/Index.html" style="text-decoration:none;">Lianwen Jin</a>â€ <br />
Proceedings of the AAAI conference on artificial intelligence (<strong>AAAI Oral</strong>), 2025<br /><br />
<a href='https://arxiv.org/abs/2412.11634'><img src='https://img.shields.io/badge/paper-9cf'></a>
<a href='https://yeungchenwa.github.io/hdr-homepage/'><img src='https://img.shields.io/badge/project-green'></a>
<a href='https://github.com/yeungchenwa/HDR'><img src='https://img.shields.io/github/stars/yeungchenwa/HDR.svg?style=social&label=Star'></a>
<br />
</font>
</p>

-----

<img style="float: left; margin:5px 10px" src="../my_images/publications/FontDiffuser.png" width="160" height="140">
### FontDiffuser: One-Shot Font Generation via Denoising Diffusion with Multi-Scale Content Aggregation and Style Contrastive Learning
<p style="line-height:1.1">
<font size="2">
<strong>Zhenhua Yang</strong>, <a href="https://scholar.google.com/citations?user=6zNgcjAAAAAJ&hl=zh-CN&oi=ao" style="text-decoration:none;">Dezhi Peng</a>, Yuxin Kong, Yuyi Zhang, <a href="https://scholar.google.com/citations?user=IpmnLFcAAAAJ&hl=zh-CN&oi=ao" style="text-decoration:none;">Cong Yao</a>, <a href="http://www.dlvc-lab.net/lianwen/Index.html" style="text-decoration:none;">Lianwen Jin</a>â€ <br />
Proceedings of the AAAI conference on artificial intelligence (<strong>AAAI</strong>), 2024<br />
<a href='https://arxiv.org/abs/2312.12142'><img src='https://img.shields.io/badge/paper-9cf'></a>
<a href='https://yeungchenwa.github.io/fontdiffuser-homepage/'><img src='https://img.shields.io/badge/project-green'></a>
<a href='https://github.com/yeungchenwa/FontDiffuser'><img src='https://img.shields.io/github/stars/yeungchenwa/FontDiffuser.svg?style=social&label=Star'></a>
<a href='https://huggingface.co/spaces/yeungchenwa/FontDiffuser-Gradio'><img src='https://img.shields.io/badge/demo-purple'></a>
<a href='https://mp.weixin.qq.com/s/DuFMB2d288eV4bHE-lyNwQ'><img src='https://img.shields.io/badge/å…¬ä¼—å·-yellow'></a>

<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/publications/UPOCR.png" width="160" height="150">
### UPOCR: Towards Unified Pixel-Level OCR Interface
<p style="line-height:1.1">
<font size="2">
<a href="https://scholar.google.com/citations?user=6zNgcjAAAAAJ&hl=zh-CN&oi=ao" style="text-decoration:none;">Dezhi Peng</a>*, <strong>Zhenhua Yang*</strong>, Jiaxin Zhang, Chongyu Liu, Yongxin Shi, <a href="http://www.dlvc-lab.net/lianwen/Index.html" style="text-decoration:none;">Lianwen Jin</a>â€ <br />International Conference on Machine Learning (<strong>ICML</strong>), 2024<br />
<a href='https://arxiv.org/abs/2312.02694'><img src='https://img.shields.io/badge/paper-9cf'></a>
<a href='https://github.com/shannanyinxiang/UPOCR'><img src='https://img.shields.io/github/stars/shannanyinxiang/UPOCR.svg?style=social&label=Star'></a>
<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/publications/megahan.png" width="160" height="160">
### MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character Recognition with over 97K Categories
<p style="line-height:1.1">
<font size="2">
Yuyi Zhang, Yongxin Shi, Peirong Zhang, Xinyi Zhang, <strong>Zhenhua Yang</strong>, <a href="http://www.dlvc-lab.net/lianwen/Index.html" style="text-decoration:none;">Lianwen Jin</a>â€ <br />
Pattern Recognition (<strong>PR</strong>), 2025<br />
<!-- <a href='https://arxiv.org/pdf/2403.13761'><img src='https://img.shields.io/badge/paper-9cf'></a> -->

<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/publications/HierCode.png" width="160" height="160">
### HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text Recognition
<p style="line-height:1.1">
<font size="2">
Yuyi Zhang, Yuanzhi Zhu, Dezhi Peng, Peirong Zhang, <strong>Zhenhua Yang</strong>, <a href="http://www.dlvc-lab.net/lianwen/Index.html" style="text-decoration:none;">Lianwen Jin</a>â€ <br />
Pattern Recognition (<strong>PR</strong>), 2024<br />
<a href='https://arxiv.org/pdf/2403.13761'><img src='https://img.shields.io/badge/paper-9cf'></a>

<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/publications/CDOR.png" width="160" height="160">
### Censoring-aware deep ordinal regression for survival prediction from pathological images
<p style="line-height:1.1">
<font size="2">
Lichao Xiao, Jin-Gang Yu, Zhifeng Liu, Jiarong Ou, Shule Deng, <strong>Zhenhua Yang</strong>, <a href="https://scholar.google.com/citations?hl=zh-CN&user=wN3v1coAAAAJ" style="text-decoration:none;">Yuanqing Li</a><br />
Medical Image Computing and Computer Assisted Intervention, (<strong>MICCAI</strong>), 2020<br />
<a href='https://link.springer.com/chapter/10.1007/978-3-030-59722-1_43'><img src='https://img.shields.io/badge/paper-9cf'></a>

<br />
</font>
</p>


Open-Source Projects
-----
<img style="float: left; margin:5px 10px" src="../my_images/projects/OCR-SAM.png" width="160" height="160">
### Optical Character Recognition with Segment Anything (OCR-SAM)
<p style="line-height:1.1">
<font size="2">
<strong>Zhenhua Yang</strong>, Qing Jiang<br />
Can SAM be applied to OCR? We take a simple try to combine two off-the-shelf OCR models in MMOCR with SAM to develop some OCR-related application demos, including SAM for Text, Text Removal and Text Inpainting. And we also provide a WebUI by gradio to give a better interaction.<br />
<a href='https://github.com/yeungchenwa/OCR-SAM'><img src='https://img.shields.io/github/stars/yeungchenwa/OCR-SAM.svg?style=social&label=Star'></a>

<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/projects/FontDiffuser.png" width="160" height="160">
### FontDiffuser: One-Shot Font Generation via Denoising Diffusion
<p style="line-height:1.1">
<font size="2">
<strong>Zhenhua Yang</strong><br />
We propose FontDiffuser, which is capable to generate unseen characters and styles, and it can be extended to the cross-lingual generation, such as Chinese to Korean.<br />
<a href='https://github.com/yeungchenwa/FontDiffuser'><img src='https://img.shields.io/github/stars/yeungchenwa/FontDiffuser.svg?style=social&label=Star'></a>

<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/projects/Recommendations-Diffusion-Text-Image.png" width="160" height="160">
### Recommendations of Diffusion for Text-Image
<p style="line-height:1.1">
<font size="2">
<strong>Zhenhua Yang</strong><br />
A paper collection of recent diffusion models for text-image generation tasks, e,g., visual text generation, font generation, text removal, text image super resolution, text editing, handwritten generation, scene text recognition and scene text detection.<br />
<a href='https://github.com/yeungchenwa/Recommendations-Diffusion-Text-Image'><img src='https://img.shields.io/github/stars/yeungchenwa/Recommendations-Diffusion-Text-Image.svg?style=social&label=Star'></a>

<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/projects/Recommendations-Document-Image-Processing.png" width="160" height="160">
### Recommendations of Document Image Processing
<p style="line-height:1.1">
<font size="2">
Jiaxin Zhang, <strong>Zhenhua Yang</strong><br />
A paper collection of the methods for document image processing, including appearance enhancement, deshadow, dewarping, deblur, and binarization.<br />
<a href='https://github.com/ZZZHANG-jx/Recommendations-Document-Image-Processing'><img src='https://img.shields.io/github/stars/ZZZHANG-jx/Recommendations-Document-Image-Processing.svg?style=social&label=Star'></a>

<br />
</font>
</p>

-----

Award
-----
- Shenzhen HighPower Technology Scholarship, 2022. (Top 2%)
- First-Class Campus Scholarship, 2021. (Top 5%)
- Second-Class Campus Scholarship, 2020. (Top 10%)
- American Mathematical Contest in Modeling, Meritorious Prize, 2020
- Alibaba Tianchi Competition of Tile Defeat Detection, Top 1.2%, 2021


Blogs
-----
[SAM(Segment-Anything)åœ¨OCRæ–‡æœ¬å›¾åƒé¢†åŸŸçš„å¯è§†åŒ–æ•ˆæœåŠç®€å•åˆ†æ](https://www.zhihu.com/question/593914819/answer/2976012032)  
[2020å¹´ç¾èµ›å¿ƒå¾—](https://www.zhihu.com/question/268052818/answer/1185708631)


Mics
-----
**Hobby**: Love a lot of sports, like FishingğŸ£, SwimmingğŸŠâ€â™‚ï¸, Riding CarğŸš², Table tennisğŸ±ğŸ“, BallğŸ€âš½ï¸, BadmintonğŸ¸ and SingingğŸ¤. I am learning to play the pianoğŸ¹ currently.  
**Game Award**: Our college team won the first-place in campus basketball gamesğŸ€ğŸ† twice when I was an undergradauate, spending the wonderfull time in my life.  
**Languange**: Chinese, English, Cantonese, and Hakka.  
**Habit**: A heavy coffee drinker â˜•ï¸~  

<table style="width: 50%; max-width: 600px" align="center" border="0" cellpadding="20">
    <!-- <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=TuVRvKdiJmKZ6t0SuQ7FsO2avdiz4cQm8yys_lX28-Q&cl=ffffff&w=a"></script> -->
    <!-- <script type="text/javascript" id="mmvst_globe" src="//mapmyvisitors.com/globe.js?d=W6xsx5HPxNJXa7j8kFXnCZ9IPsDYpfC2Dq33fbsee5Q"></script> -->
    <script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffffff&w=a&t=tt&d=TuVRvKdiJmKZ6t0SuQ7FsO2avdiz4cQm8yys_lX28-Q&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
</table>
