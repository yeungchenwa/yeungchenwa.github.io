---
permalink: /
title: About Me
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<style>
body, h1, h2, h3, h4, h5, h6 {
  font-family: 'Times New Roman', Times, serif;
}
</style>

My name is Zhenhua Yang (æ¨æŒ¯å, Yeung Chenwa), I'm a second-year Master's student from <a href="https://github.com/HCIILAB" style="text-decoration:none;">SCUT-DLVCLab</a> in <a href="https://www2.scut.edu.cn/ee/" style="text-decoration:none;">School of Electronic and Information Engineering</a>, <a href="https://www.scut.edu.cn/new/" style="text-decoration:none;">South China University of Technology</a>, supervised by <a href="http://www.dlvc-lab.net/lianwen/Index.html" style="text-decoration:none;">Prof. Lianwen Jin</a>. 
I received my Bachelor degree from <a href="https://www2.scut.edu.cn/automation/" style="text-decoration:none;">School of Automation Science and Engineering</a>, <a href="https://www.scut.edu.cn/new/" style="text-decoration:none;">South China University of Technology</a> in 2022. 
<!-- I also works closely with <a href="https://scholar.google.com/citations?user=6zNgcjAAAAAJ&hl=zh-CN&oi=ao" style="text-decoration:none;">Dezhi Peng</a> now.  -->

My research interests are focused on Diffusion Model, Image Generation, Document Restoration, and Video Generation. I am also devoted into the open source community.  

<!-- <p style="color: red;">I'm looking for a 2025Fall PhD position!</p> -->

<p align="center">
<a href='https://github.com/yeungchenwa'>GitHub</a> / 
<a href='https://scholar.google.com/citations?hl=zh-CN&user=2ITs6lUAAAAJ'>Google Scholar</a> / 
<a href='eezhyang@gmail.com'>Email</a> / 
<a href='https://www.zhihu.com/people/young-40-31'>Zhihu</a>
</p>

News
-----
âˆ™ **[12/2023]** ğŸ”¥ğŸ”¥ğŸ”¥ The ğŸ“º<a href="https://huggingface.co/spaces/yeungchenwa/FontDiffuser-Gradio" style="text-decoration:none;">Hugging Face Demo</a> and the ğŸ§‘â€ğŸ’»<a href="https://github.com/yeungchenwa/FontDiffuser" style="text-decoration:none;">Github Repository</a> of <strong><a href='https://arxiv.org/abs/2312.12142'>FontDiffuser</a></strong> is released! Welcome to check it out.  
âˆ™ **[12/2023]** ğŸ‰ The paper <a href="https://arxiv.org/abs/2312.12142" style="text-decoration:none;">FontDiffuser</a> is accepted by <strong>AAAI2024</strong>, which excels in complex character generation and large style variation. The code and demo will be released soon.<br />
âˆ™ **[12/2023]** Our paper <a href="https://arxiv.org/abs/2312.02694" style="text-decoration:none;">UPOCR</a> is released to arXiv.<br />

Education
-----
<img style="float: left; margin:5px 10px" src="../my_images/SCUT_logo.png" width="100" height="100">
### South China University of Technology
<p style="line-height:1.1">
<font size="2">
Sep. 2022 - Present<br />
M.S student at <a href="https://github.com/HCIILAB" style="text-decoration:none;">SCUT-DLVCLab</a> in <a href="https://www2.scut.edu.cn/ee/" style="text-decoration:none;">School of Electronic and Information Engineering</a><br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/SCUT_logo.png" width="100" height="100">
### South China University of Technology
<p style="line-height:1.1">
<font size="2">
Sep. 2018 - Jun. 2022 <br />
B.E student in <a href="https://www2.scut.edu.cn/automation/" style="text-decoration:none;">School of Automation Science and Engineering</a><br />
</font>
</p>

Publications
-----
<img style="float: left; margin:5px 10px" src="../my_images/publications/FontDiffuser.png" width="160" height="160">
### FontDiffuser: One-Shot Font Generation via Denoising Diffusion with Multi-Scale Content Aggregation and Style Contrastive Learning
<p style="line-height:1.1">
<font size="2">
<strong>Zhenhua Yang</strong>, <a href="https://scholar.google.com/citations?user=6zNgcjAAAAAJ&hl=zh-CN&oi=ao" style="text-decoration:none;">Dezhi Peng</a>, Yuxin Kong, Yuyi Zhang, <a href="https://scholar.google.com/citations?user=IpmnLFcAAAAJ&hl=zh-CN&oi=ao" style="text-decoration:none;">Cong Yao</a>, <a href="http://www.dlvc-lab.net/lianwen/Index.html" style="text-decoration:none;">Lianwen Jin</a>â€ <br />
Proceedings of the AAAI conference on artificial intelligence (<strong>AAAI</strong>), 2024<br />
<a href='https://arxiv.org/abs/2312.12142'><img src='https://img.shields.io/badge/paper-9cf'></a>
<a href='https://yeungchenwa.github.io/fontdiffuser-homepage/'><img src='https://img.shields.io/badge/project-green'></a>
<a href='https://github.com/yeungchenwa/FontDiffuser'><img src='https://img.shields.io/github/stars/yeungchenwa/FontDiffuser.svg?style=social&label=Star'></a>
<a href='https://huggingface.co/spaces/yeungchenwa/FontDiffuser-Gradio'><img src='https://img.shields.io/badge/demo-purple'></a>
<a href='https://mp.weixin.qq.com/s/DuFMB2d288eV4bHE-lyNwQ'><img src='https://img.shields.io/badge/å…¬ä¼—å·-yellow'></a>

<br />
</font>
</p>

<img style="float: left; margin:5px 10px" src="../my_images/publications/UPOCR.png" width="160" height="160">
### UPOCR: Towards Unified Pixel-Level OCR Interface
<p style="line-height:1.1">
<font size="2">
<a href="https://scholar.google.com/citations?user=6zNgcjAAAAAJ&hl=zh-CN&oi=ao" style="text-decoration:none;">Dezhi Peng</a>*, <strong>Zhenhua Yang*</strong>, Jiaxin Zhang, Chongyu Liu, Yongxin Shi, <a href="http://www.dlvc-lab.net/lianwen/Index.html" style="text-decoration:none;">Lianwen Jin</a>â€ <br />
arXiv Preprint, 2023<br />
<a href='https://arxiv.org/abs/2312.02694'><img src='https://img.shields.io/badge/paper-9cf'></a>

<br />
</font>
</p>

<!-- -----
<img style="float: left; margin:5px 10px" src="../my_images/publications/CDOR.png" width="160" height="160">
### Censoring-aware deep ordinal regression for survival prediction from pathological images
<p style="line-height:1.1">
<font size="2">
Lichao Xiao, Jin-Gang Yu, Zhifeng Liu, Jiarong Ou, Shule Deng, <strong>Zhenhua Yang</strong>, <a href="https://scholar.google.com/citations?hl=zh-CN&user=wN3v1coAAAAJ" style="text-decoration:none;">Yuanqing Li</a><br />
Medical Image Computing and Computer Assisted Intervention, (<strong>MICCAI</strong>), 2020<br />
<a href='https://link.springer.com/chapter/10.1007/978-3-030-59722-1_43'><img src='https://img.shields.io/badge/paper-9cf'></a>

<br />
</font>
</p> -->


Open-Source Projects
-----
<img style="float: left; margin:5px 10px" src="../my_images/projects/OCR-SAM.png" width="160" height="160">
### Optical Character Recognition with Segment Anything (OCR-SAM)
<p style="line-height:1.1">
<font size="2">
<strong>Zhenhua Yang</strong>, Qing Jiang<br />
Can SAM be applied to OCR? We take a simple try to combine two off-the-shelf OCR models in MMOCR with SAM to develop some OCR-related application demos, including SAM for Text, Text Removal and Text Inpainting. And we also provide a WebUI by gradio to give a better interaction.<br />
<a href='https://github.com/yeungchenwa/OCR-SAM'><img src='https://img.shields.io/github/stars/yeungchenwa/OCR-SAM.svg?style=social&label=Star'></a>

<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/projects/FontDiffuser.png" width="160" height="160">
### FontDiffuser: One-Shot Font Generation via Denoising Diffusion
<p style="line-height:1.1">
<font size="2">
<strong>Zhenhua Yang</strong><br />
We propose FontDiffuser, which is capable to generate unseen characters and styles, and it can be extended to the cross-lingual generation, such as Chinese to Korean.<br />
<a href='https://github.com/yeungchenwa/FontDiffuser'><img src='https://img.shields.io/github/stars/yeungchenwa/FontDiffuser.svg?style=social&label=Star'></a>

<br />
</font>
</p>

-----
<img style="float: left; margin:5px 10px" src="../my_images/projects/Recommendations-Diffusion-Text-Image.png" width="160" height="160">
### Recommendations of Diffusion for Text-Image
<p style="line-height:1.1">
<font size="2">
<strong>Zhenhua Yang</strong><br />
A paper collection of recent diffusion models for text-image generation tasks, e,g., visual text generation, font generation, text removal, text image super resolution, text editing, handwritten generation, scene text recognition and scene text detection.<br />
<a href='https://github.com/yeungchenwa/Recommendations-Diffusion-Text-Image'><img src='https://img.shields.io/github/stars/yeungchenwa/Recommendations-Diffusion-Text-Image.svg?style=social&label=Star'></a>

<br />
</font>
</p>


Award
-----
- Shenzhen HighPower Technology Scholarship, 2022. (Top 2%)
- First-Class Campus Scholarship, 2021. (Top 5%)
- Second-Class Campus Scholarship, 2020. (Top 10%)
- American Mathematical Contest in Modeling, Meritorious Prize, 2020
- Alibaba Tianchi Competition of Tile Defeat Detection, Top 1.2%, 2021


Blogs
-----
[SAM(Segment-Anything)åœ¨OCRæ–‡æœ¬å›¾åƒé¢†åŸŸçš„å¯è§†åŒ–æ•ˆæœåŠç®€å•åˆ†æ](https://www.zhihu.com/question/593914819/answer/2976012032)  
[2020å¹´ç¾èµ›å¿ƒå¾—](https://www.zhihu.com/question/268052818/answer/1185708631)


Mics
-----
**Hobby**: Love a lot of sports, like FishingğŸ£, SwimmingğŸŠâ€â™‚ï¸, Riding CarğŸš², Table tennisğŸ±ğŸ“, BallğŸ€âš½ï¸, BadmintonğŸ¸ and SingingğŸ¤. I am learning to play the pianoğŸ¹ currently.  
**Game Award**: Our college team won the first-place in campus basketball gamesğŸ€ğŸ† twice when I was an undergradauate, spending the wonderfull time in my life.  
**Languange**: Chinese, English, Cantonese, and Hakka.  
**Habit**: A heavy coffee drinker â˜•ï¸~  

<table style="width: 50%; max-width: 600px" align="center" border="0" cellpadding="20">
    <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=TuVRvKdiJmKZ6t0SuQ7FsO2avdiz4cQm8yys_lX28-Q&cl=ffffff&w=a"></script>
    <!-- <script type="text/javascript" id="mmvst_globe" src="//mapmyvisitors.com/globe.js?d=W6xsx5HPxNJXa7j8kFXnCZ9IPsDYpfC2Dq33fbsee5Q"></script> -->
</table>
